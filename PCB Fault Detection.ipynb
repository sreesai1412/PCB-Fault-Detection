{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "Vkl37VKlFpBm",
    "outputId": "c69a3b10-c209-41f3-d2f3-c7edbd3f532e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Klm-jJnMDI7R",
    "outputId": "deee48d6-618e-452d-fb95-8e1f5fa19425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "fjvgSViEDbNi",
    "outputId": "a4590cfc-00da-40a8-af67-8df643847ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.backend.clear_session()\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PQZ7yPB0GLFi",
    "outputId": "0c5772d7-0d4c-4928-bf40-272d13fb6c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "YcjPTdeaFpB3",
    "outputId": "e05d808c-7693-47cc-bbc3-2baa3e5d706f"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/PCB/Train_Data')\n",
    "train_x = np.load('train_X.npy')\n",
    "train_x = train_x[:,100:200,100:200,:]\n",
    "train_y = np.load('train_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "iA4YOk_ct8rB",
    "outputId": "bfe85b68-fd8e-4b94-bb1b-2f3fe9a9efd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "train_y = tf.convert_to_tensor(train_y, np.float32)\n",
    "sess = tf.InteractiveSession()\n",
    "print(train_y.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MP7n3FcZFpB9",
    "outputId": "4ac731e0-849d-4d65-fe4c-71049f28f142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([15000, 15000]))\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "print(np.unique(train_y,return_counts=True))\n",
    "train_y= np_utils.to_categorical(train_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "7Mwoa6PBFpCF",
    "outputId": "d883d07e-ee36-42ca-ba4b-68a4f327a80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n",
      "(30000, 100, 100, 3)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9r91tpe5FpCM",
    "outputId": "0cfcc1ff-e06c-4471-b3c7-2427f36865ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.34926454777778 58.207460392701165\n",
      "(30000, 100, 100, 3)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(train_x)\n",
    "std = np.std(train_x)\n",
    "print (mean, std)\n",
    "\n",
    "train_x = train_x - mean\n",
    "train_x = train_x / std\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "GGDKVQlqFpCT",
    "outputId": "302e9383-fb71-49af-83b6-ceecb1335465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "24000\n",
      "\n",
      "6000\n",
      "6000\n",
      "\n",
      "(24000, 100, 100, 3)\n",
      "(24000, 2)\n",
      "\n",
      "(6000, 100, 100, 3)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(len(X_valid))\n",
    "print(len(y_valid))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "9_-VfALgFpCZ",
    "outputId": "6e0290e6-549a-4e29-ca01-4b569da60ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 100, 100, 3)\n",
      "(24000, 2)\n",
      "\n",
      "(6000, 100, 100, 3)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "#y_train = np.reshape(y_train,(-1,1))\n",
    "\n",
    "X_valid = X_valid.astype('float32')\n",
    "y_valid = y_valid.astype('float32')\n",
    "#y_valid = np.reshape(y_valid,(-1,1))\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"\")\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the models\n",
    "Here all the different ResNet Models(18,34,50,101,152) are defined.\n",
    "In this notebook however,the ResNet34 model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_pfeBVduFpCd",
    "outputId": "f97f5756-d8e7-47f0-80ef-52d183a18f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 50, 50, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 50, 50, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 50, 50, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 25, 25, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 25, 25, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 25, 25, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 25, 25, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 25, 25, 64)   0           activation_3[0][0]               \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 25, 25, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 25, 25, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 25, 25, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 25, 25, 64)   0           activation_6[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 25, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 25, 25, 64)   0           activation_9[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 64)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 13, 13, 128)  73856       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 13, 13, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 13, 13, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 13, 13, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 13, 13, 128)  8320        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 13, 13, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 13, 13, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 13, 13, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 13, 13, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 13, 13, 128)  0           activation_12[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 13, 13, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 13, 13, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 13, 13, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 13, 13, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 13, 13, 128)  147584      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 13, 13, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 13, 13, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 13, 13, 128)  0           activation_16[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 13, 13, 128)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 13, 13, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 13, 13, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 13, 13, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 13, 13, 128)  147584      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 13, 13, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 13, 13, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 13, 13, 128)  0           activation_19[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 13, 13, 128)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 13, 13, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 13, 13, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 13, 13, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 13, 13, 128)  147584      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 13, 13, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 13, 13, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 13, 13, 128)  0           activation_22[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 13, 13, 128)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 7, 7, 256)    295168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 7, 7, 256)    1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 7, 7, 256)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 256)    590080      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 7, 7, 256)    33024       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 7, 7, 256)    1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 7, 7, 256)    1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 7, 7, 256)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 7, 7, 256)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 7, 7, 256)    0           activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 7, 7, 256)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 7, 7, 256)    590080      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 7, 7, 256)    1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 7, 7, 256)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 7, 7, 256)    590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 7, 7, 256)    1024        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 7, 7, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 7, 7, 256)    0           activation_29[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 7, 7, 256)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 7, 7, 256)    590080      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 7, 7, 256)    1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 7, 7, 256)    590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 7, 7, 256)    1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 256)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 7, 7, 256)    0           activation_32[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 256)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 7, 7, 256)    590080      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 7, 7, 256)    1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 256)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 7, 7, 256)    590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 7, 7, 256)    1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 7, 7, 256)    0           activation_35[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 256)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 7, 7, 256)    590080      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 7, 7, 256)    1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 256)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 7, 7, 256)    590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 7, 7, 256)    1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 7, 7, 256)    0           activation_38[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 7, 7, 256)    0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 7, 7, 256)    590080      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 7, 7, 256)    1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 256)    590080      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 7, 7, 256)    1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 256)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 256)    0           activation_41[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 256)    0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 512)    1180160     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 512)    2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 512)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 512)    131584      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 512)    2048        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 512)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 512)    0           activation_44[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 512)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 512)    2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 512)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 512)    2048        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 512)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 512)    0           activation_48[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 512)    0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 512)    2359808     activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 512)    2048        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 512)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 512)    2359808     activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 4, 4, 512)    2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 512)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 512)    0           activation_51[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 512)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 21,311,234\n",
      "Trainable params: 21,294,210\n",
      "Non-trainable params: 17,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def _after_conv(in_tensor):\n",
    "    norm = layers.BatchNormalization()(in_tensor)\n",
    "    return layers.Activation('relu')(norm)\n",
    "\n",
    "def conv1(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=1, strides=1)(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv1_downsample(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=1, strides=2)(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv3(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv3_downsample(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same')(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def resnet_block_wo_bottlneck(in_tensor, filters, downsample=False):\n",
    "    if downsample:\n",
    "        conv1_rb = conv3_downsample(in_tensor, filters)\n",
    "    else:\n",
    "        conv1_rb = conv3(in_tensor, filters)\n",
    "    conv2_rb = conv3(conv1_rb, filters)\n",
    "\n",
    "    if downsample:\n",
    "        in_tensor = conv1_downsample(in_tensor, filters)\n",
    "    result = layers.Add()([conv2_rb, in_tensor])\n",
    "\n",
    "    return layers.Activation('relu')(result)\n",
    "\n",
    "def resnet_block_w_bottlneck(in_tensor,\n",
    "                             filters,\n",
    "                             downsample=False,\n",
    "                             change_channels=False):\n",
    "    if downsample:\n",
    "        conv1_rb = conv1_downsample(in_tensor, int(filters/4))\n",
    "    else:\n",
    "        conv1_rb = conv1(in_tensor, int(filters/4))\n",
    "    conv2_rb = conv3(conv1_rb, int(filters/4))\n",
    "    conv3_rb = conv1(conv2_rb, filters)\n",
    "\n",
    "    if downsample:\n",
    "        in_tensor = conv1_downsample(in_tensor, filters)\n",
    "    elif change_channels:\n",
    "        in_tensor = conv1(in_tensor, filters)\n",
    "    result = layers.Add()([conv3_rb, in_tensor])\n",
    "\n",
    "    return result\n",
    "\n",
    "def _pre_res_blocks(in_tensor):\n",
    "    conv = layers.Conv2D(64, 7, strides=2, padding='same')(in_tensor)\n",
    "    conv = _after_conv(conv)\n",
    "    pool = layers.MaxPool2D(3, 2, padding='same')(conv)\n",
    "    return pool\n",
    "\n",
    "def _post_res_blocks(in_tensor, n_classes):\n",
    "    pool = layers.GlobalAvgPool2D()(in_tensor)\n",
    "    preds = layers.Dense(n_classes, activation='softmax')(pool)\n",
    "    return preds\n",
    "\n",
    "def convx_wo_bottleneck(in_tensor, filters, n_times, downsample_1=False):\n",
    "    res = in_tensor\n",
    "    for i in range(n_times):\n",
    "        if i == 0:\n",
    "            res = resnet_block_wo_bottlneck(res, filters, downsample_1)\n",
    "        else:\n",
    "            res = resnet_block_wo_bottlneck(res, filters)\n",
    "    return res\n",
    "\n",
    "def convx_w_bottleneck(in_tensor, filters, n_times, downsample_1=False):\n",
    "    res = in_tensor\n",
    "    for i in range(n_times):\n",
    "        if i == 0:\n",
    "            res = resnet_block_w_bottlneck(res, filters, downsample_1, not downsample_1)\n",
    "        else:\n",
    "            res = resnet_block_w_bottlneck(res, filters)\n",
    "    return res\n",
    "\n",
    "def _resnet(in_shape=(224,224,3),\n",
    "            n_classes=1000,\n",
    "            convx=[64, 128, 256, 512],\n",
    "            n_convx=[2, 2, 2, 2],\n",
    "            convx_fn=convx_wo_bottleneck):\n",
    "    in_layer = layers.Input(in_shape)\n",
    "\n",
    "    downsampled = _pre_res_blocks(in_layer)\n",
    "\n",
    "    conv2x = convx_fn(downsampled, convx[0], n_convx[0])\n",
    "    conv3x = convx_fn(conv2x, convx[1], n_convx[1], True)\n",
    "    conv4x = convx_fn(conv3x, convx[2], n_convx[2], True)\n",
    "    conv5x = convx_fn(conv4x, convx[3], n_convx[3], True)\n",
    "\n",
    "    preds = _post_res_blocks(conv5x, n_classes)\n",
    "\n",
    "    model = Model(in_layer, preds)\n",
    "    return model\n",
    "\n",
    "def resnet18(in_shape=(100,100,3), n_classes=2):\n",
    "    return _resnet(in_shape, n_classes)\n",
    "\n",
    "def resnet34(in_shape=(100,100,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,n_convx=[3, 4, 6, 3])\n",
    "\n",
    "def resnet50(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 4, 6, 3],convx_w_bottleneck)\n",
    "\n",
    "def resnet101(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 4, 23, 3],convx_w_bottleneck)\n",
    "\n",
    "def resnet152(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 8, 36, 3],convx_w_bottleneck)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = resnet34()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfbVtx5YFpCi"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/PCB/Models/Res34/Results')\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler,EarlyStopping\n",
    "mc = ModelCheckpoint('PCB3_Res34_Aug_data_100*100.h5', monitor='val_loss', mode='min',save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=25, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "cv = CSVLogger('PCB3_Res34_Aug_data_100*100.csv',append=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftLuapKPFpCl"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qd_Vh6PkFpCr",
    "outputId": "9aa3becf-2f9b-4dcc-8b95-7d9331ca2c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 62s 3ms/step - loss: 0.4952 - acc: 0.8045 - val_loss: 1.3657 - val_acc: 0.6108\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.2999 - acc: 0.8788 - val_loss: 3.3351 - val_acc: 0.5848\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.2202 - acc: 0.9160 - val_loss: 0.2824 - val_acc: 0.8855\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1878 - acc: 0.9288 - val_loss: 0.2819 - val_acc: 0.8873\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1635 - acc: 0.9384 - val_loss: 0.1549 - val_acc: 0.9412\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1468 - acc: 0.9459 - val_loss: 0.3512 - val_acc: 0.8805\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1348 - acc: 0.9508 - val_loss: 0.1556 - val_acc: 0.9403\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1722 - acc: 0.9374 - val_loss: 0.1668 - val_acc: 0.9370\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1241 - acc: 0.9539 - val_loss: 0.2189 - val_acc: 0.9180\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1063 - acc: 0.9604 - val_loss: 0.1261 - val_acc: 0.9525\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1082 - acc: 0.9599 - val_loss: 0.1281 - val_acc: 0.9498\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1078 - acc: 0.9598 - val_loss: 0.1282 - val_acc: 0.9498\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0880 - acc: 0.9665 - val_loss: 0.1179 - val_acc: 0.9557\n",
      "Epoch 14/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0797 - acc: 0.9707 - val_loss: 0.1249 - val_acc: 0.9533\n",
      "Epoch 15/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0737 - acc: 0.9729 - val_loss: 0.2134 - val_acc: 0.9237\n",
      "Epoch 16/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0669 - acc: 0.9754 - val_loss: 0.1669 - val_acc: 0.9420\n",
      "Epoch 17/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0677 - acc: 0.9748 - val_loss: 0.1486 - val_acc: 0.9505\n",
      "Epoch 18/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0597 - acc: 0.9787 - val_loss: 0.1298 - val_acc: 0.9565\n",
      "Epoch 19/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0566 - acc: 0.9793 - val_loss: 0.3273 - val_acc: 0.9380\n",
      "Epoch 20/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0520 - acc: 0.9821 - val_loss: 0.1401 - val_acc: 0.9512\n",
      "Epoch 21/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0536 - acc: 0.9810 - val_loss: 0.1515 - val_acc: 0.9562\n",
      "Epoch 22/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0478 - acc: 0.9826 - val_loss: 0.1510 - val_acc: 0.9555\n",
      "Epoch 23/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0424 - acc: 0.9855 - val_loss: 0.1732 - val_acc: 0.9528\n",
      "Epoch 24/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0398 - acc: 0.9864 - val_loss: 0.1705 - val_acc: 0.9515\n",
      "Epoch 25/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0553 - acc: 0.9803 - val_loss: 0.1529 - val_acc: 0.9487\n",
      "Epoch 26/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0397 - acc: 0.9852 - val_loss: 0.1463 - val_acc: 0.9550\n",
      "Epoch 27/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0402 - acc: 0.9861 - val_loss: 0.1463 - val_acc: 0.9570\n",
      "Epoch 28/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0306 - acc: 0.9889 - val_loss: 0.2046 - val_acc: 0.9487\n",
      "Epoch 29/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0281 - acc: 0.9900 - val_loss: 0.1748 - val_acc: 0.9552\n",
      "Epoch 30/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0293 - acc: 0.9890 - val_loss: 0.1779 - val_acc: 0.9548\n",
      "Epoch 31/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0310 - acc: 0.9896 - val_loss: 0.2385 - val_acc: 0.9403\n",
      "Epoch 32/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0280 - acc: 0.9898 - val_loss: 0.2102 - val_acc: 0.9455\n",
      "Epoch 33/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0279 - acc: 0.9897 - val_loss: 0.1605 - val_acc: 0.9598\n",
      "Epoch 34/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0266 - acc: 0.9903 - val_loss: 1.2504 - val_acc: 0.7722\n",
      "Epoch 35/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0259 - acc: 0.9911 - val_loss: 0.1518 - val_acc: 0.9598\n",
      "Epoch 36/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0238 - acc: 0.9918 - val_loss: 0.2381 - val_acc: 0.9382\n",
      "Epoch 37/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0236 - acc: 0.9911 - val_loss: 0.1911 - val_acc: 0.9568\n",
      "Epoch 38/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0220 - acc: 0.9919 - val_loss: 0.1961 - val_acc: 0.9558\n",
      "Epoch 39/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0205 - acc: 0.9926 - val_loss: 0.1836 - val_acc: 0.9572\n",
      "Epoch 40/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0232 - acc: 0.9925 - val_loss: 0.1988 - val_acc: 0.9538\n",
      "Epoch 41/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0182 - acc: 0.9935 - val_loss: 0.4525 - val_acc: 0.9198\n",
      "Epoch 42/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0195 - acc: 0.9932 - val_loss: 0.2102 - val_acc: 0.9517\n",
      "Epoch 43/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0190 - acc: 0.9930 - val_loss: 0.1820 - val_acc: 0.9573\n",
      "Epoch 44/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0216 - acc: 0.9928 - val_loss: 0.1957 - val_acc: 0.9553\n",
      "Epoch 45/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0171 - acc: 0.9945 - val_loss: 0.2111 - val_acc: 0.9553\n",
      "Epoch 46/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0194 - acc: 0.9933 - val_loss: 0.1920 - val_acc: 0.9543\n",
      "Epoch 47/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0195 - acc: 0.9938 - val_loss: 0.2252 - val_acc: 0.9518\n",
      "Epoch 48/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0261 - acc: 0.9919 - val_loss: 4.0476 - val_acc: 0.7153\n",
      "Epoch 49/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.1714 - acc: 0.9630 - val_loss: 0.2753 - val_acc: 0.9402\n",
      "Epoch 50/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0303 - acc: 0.9892 - val_loss: 0.3842 - val_acc: 0.9445\n",
      "Epoch 51/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0162 - acc: 0.9942 - val_loss: 0.3575 - val_acc: 0.9465\n",
      "Epoch 52/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0167 - acc: 0.9945 - val_loss: 0.3269 - val_acc: 0.9478\n",
      "Epoch 53/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0129 - acc: 0.9953 - val_loss: 0.4142 - val_acc: 0.9423\n",
      "Epoch 54/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.3889 - val_acc: 0.9467\n",
      "Epoch 55/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0137 - acc: 0.9951 - val_loss: 0.2654 - val_acc: 0.9537\n",
      "Epoch 56/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.2577 - val_acc: 0.9527\n",
      "Epoch 57/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0155 - acc: 0.9947 - val_loss: 0.2243 - val_acc: 0.9572\n",
      "Epoch 58/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0148 - acc: 0.9946 - val_loss: 0.1894 - val_acc: 0.9563\n",
      "Epoch 59/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0112 - acc: 0.9957 - val_loss: 0.2586 - val_acc: 0.9543\n",
      "Epoch 60/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.2770 - val_acc: 0.9468\n",
      "Epoch 61/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0120 - acc: 0.9955 - val_loss: 0.2085 - val_acc: 0.9585\n",
      "Epoch 62/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.2446 - val_acc: 0.9522\n",
      "Epoch 63/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0113 - acc: 0.9960 - val_loss: 0.1956 - val_acc: 0.9615\n",
      "Epoch 64/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.2095 - val_acc: 0.9608\n",
      "Epoch 65/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0117 - acc: 0.9964 - val_loss: 0.2424 - val_acc: 0.9523\n",
      "Epoch 66/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0093 - acc: 0.9968 - val_loss: 0.2348 - val_acc: 0.9585\n",
      "Epoch 67/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.2007 - val_acc: 0.9578\n",
      "Epoch 68/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.2800 - val_acc: 0.9528\n",
      "Epoch 69/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.2494 - val_acc: 0.9553\n",
      "Epoch 70/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0092 - acc: 0.9966 - val_loss: 0.2438 - val_acc: 0.9595\n",
      "Epoch 71/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.2638 - val_acc: 0.9532\n",
      "Epoch 72/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 0.2020 - val_acc: 0.9573\n",
      "Epoch 73/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.2305 - val_acc: 0.9583\n",
      "Epoch 74/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.2430 - val_acc: 0.9540\n",
      "Epoch 75/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.2546 - val_acc: 0.9560\n",
      "Epoch 76/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.2449 - val_acc: 0.9580\n",
      "Epoch 77/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0082 - acc: 0.9966 - val_loss: 0.2422 - val_acc: 0.9560\n",
      "Epoch 78/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.2321 - val_acc: 0.9582\n",
      "Epoch 79/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.5327 - val_acc: 0.9003\n",
      "Epoch 80/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.2720 - val_acc: 0.9535\n",
      "Epoch 81/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0058 - acc: 0.9977 - val_loss: 0.2665 - val_acc: 0.9582\n",
      "Epoch 82/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0079 - acc: 0.9972 - val_loss: 0.2592 - val_acc: 0.9557\n",
      "Epoch 83/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.2904 - val_acc: 0.9535\n",
      "Epoch 84/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.2526 - val_acc: 0.9575\n",
      "Epoch 85/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.3279 - val_acc: 0.9410\n",
      "Epoch 86/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.2359 - val_acc: 0.9572\n",
      "Epoch 87/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0069 - acc: 0.9980 - val_loss: 0.2620 - val_acc: 0.9552\n",
      "Epoch 88/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.2281 - val_acc: 0.9603\n",
      "Epoch 89/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.2452 - val_acc: 0.9580\n",
      "Epoch 90/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0074 - acc: 0.9973 - val_loss: 0.2838 - val_acc: 0.9465\n",
      "Epoch 91/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.2275 - val_acc: 0.9563\n",
      "Epoch 92/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0075 - acc: 0.9974 - val_loss: 0.2359 - val_acc: 0.9562\n",
      "Epoch 93/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.2348 - val_acc: 0.9552\n",
      "Epoch 94/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0042 - acc: 0.9981 - val_loss: 0.2826 - val_acc: 0.9497\n",
      "Epoch 95/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0073 - acc: 0.9974 - val_loss: 0.2642 - val_acc: 0.9537\n",
      "Epoch 96/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.2548 - val_acc: 0.9573\n",
      "Epoch 97/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.2799 - val_acc: 0.9572\n",
      "Epoch 98/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.2925 - val_acc: 0.9515\n",
      "Epoch 99/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.2927 - val_acc: 0.9553\n",
      "Epoch 100/100\n",
      "24000/24000 [==============================] - 47s 2ms/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.2049 - val_acc: 0.9600\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_valid,y_valid),verbose = 1,shuffle=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UjlspN_5FpCw",
    "outputId": "f258d422-15bd-4929-82fc-0774cebd86d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 300, 300, 3)\n",
      "(2838,)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/content/drive/My Drive/PCB/Test_Data')\n",
    "\n",
    "unseen_d = np.load('test_X.npy')\n",
    "unseen_l = np.load('test_y.npy')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(unseen_d, unseen_l, test_size=0.2, shuffle= True)\n",
    "\n",
    "print(unseen_d.shape)\n",
    "print(unseen_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "82l0zM5LJ7hv",
    "outputId": "4edffaa7-abd3-4cca-fe2e-fd1648da8b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "unseen_d=unseen_d[:,100:200,100:200,:];\n",
    "print(unseen_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LJd_4FYqKAVp",
    "outputId": "99f5de7c-15e7-4b7b-ae07-3ef9de4b492b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "unseen_d = unseen_d.reshape(unseen_d.shape[0],100,100,3).astype('float32')\n",
    "mean=62.34\n",
    "std=58.20\n",
    " \n",
    "unseen_d = unseen_d - mean\n",
    "unseen_d = unseen_d / std\n",
    "print (unseen_d.shape)\n",
    "unseen_d = unseen_d.astype('float32')\n",
    "unseen_l = unseen_l.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-9-3r-MyKDrb",
    "outputId": "b1e44929-0b55-4aa7-d510-5966d9865939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.], dtype=float32), array([  75, 2763]))\n",
      "(2838, 2)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "print(np.unique(unseen_l,return_counts=True))\n",
    "unseen_l= np_utils.to_categorical(unseen_l, num_classes)\n",
    "print(unseen_l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Sy-N7wJBKIA3",
    "outputId": "f03bee50-9029-4585-85e8-fc7aae975a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2838/2838 [==============================] - 2s 537us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5271311750119634, 0.7357293869341823]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(unseen_d,unseen_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "qbNAFob8KLDT",
    "outputId": "9fcfd677-b994-4ddf-c129-76a1b3e48d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Deffect 0       0.08      0.80      0.14        75\n",
      "Non deffect 1       0.99      0.73      0.84      2763\n",
      "\n",
      "     accuracy                           0.74      2838\n",
      "    macro avg       0.53      0.77      0.49      2838\n",
      " weighted avg       0.97      0.74      0.83      2838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix, precision_score\n",
    "import itertools\n",
    "\n",
    "Y_pred = model.predict(unseen_d)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "target_names = ['Deffect 0', 'Non deffect 1']\n",
    "print(classification_report(np.argmax(unseen_l,axis=1), y_pred,target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgGqDb8NKOUO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    acc=np.trace(cm)/float(np.sum(cm))\n",
    "    miss_class=1-acc\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    print(acc)\n",
    "    print(miss_class)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i+0.25 if i==0 else i-0.25, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "colab_type": "code",
    "id": "rNTU6eg_KR_l",
    "outputId": "448663b2-9753-4c25-bd98-3722aa9b1b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[  60   15]\n",
      " [ 735 2028]]\n",
      "0.7357293868921776\n",
      "0.2642706131078224\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAK7CAYAAADyY0eCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsP4fSmB7Bk4O5+IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnf_matrix = (confusion_matrix(np.argmax(unseen_l,axis=1), y_pred))\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Res34.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
