{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/student/Documents/PCB_3/Train_Data')\n",
    "train_x = np.load('train_X.npy')\n",
    "train_x = train_x[:,100:200,100:200,:];\n",
    "train_y = np.load('train_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([15000, 15000]))\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "print(np.unique(train_y,return_counts=True))\n",
    "train_y= np_utils.to_categorical(train_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n",
      "(30000, 100, 100, 3)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.34926454777778 58.207460392701165\n",
      "(30000, 100, 100, 3)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(train_x)\n",
    "std = np.std(train_x)\n",
    "print (mean, std)\n",
    "\n",
    "train_x = train_x - mean\n",
    "train_x = train_x / std\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "24000\n",
      "\n",
      "6000\n",
      "6000\n",
      "\n",
      "(24000, 100, 100, 3)\n",
      "(24000, 2)\n",
      "\n",
      "(6000, 100, 100, 3)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(len(X_valid))\n",
    "print(len(y_valid))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 100, 100, 3)\n",
      "(24000, 2)\n",
      "\n",
      "(6000, 100, 100, 3)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "#y_train = np.reshape(y_train,(-1,1))\n",
    "\n",
    "X_valid = X_valid.astype('float32')\n",
    "y_valid = y_valid.astype('float32')\n",
    "#y_valid = np.reshape(y_valid,(-1,1))\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"\")\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 10:14:54.871383 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1209 10:14:54.887747 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1209 10:14:54.891199 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1209 10:14:54.914226 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1209 10:14:54.915233 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1209 10:14:55.838974 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1209 10:14:55.904125 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 50, 50, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 50, 50, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 50, 50, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 25, 25, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 25, 25, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 25, 25, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 25, 25, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 25, 25, 64)   0           activation_3[0][0]               \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 25, 25, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 25, 25, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 25, 25, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 25, 25, 64)   0           activation_6[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 13, 13, 128)  73856       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 13, 13, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 13, 13, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 13, 13, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 13, 13, 128)  8320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 13, 13, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 13, 13, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 13, 13, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 13, 13, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 13, 13, 128)  0           activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 13, 13, 128)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 13, 13, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 13, 13, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 13, 13, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 13, 13, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 13, 13, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 13, 13, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 13, 13, 128)  0           activation_13[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 13, 13, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 7, 7, 256)    295168      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 7, 7, 256)    1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 7, 7, 256)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 256)    590080      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 7, 7, 256)    33024       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 7, 7, 256)    1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 7, 7, 256)    1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 7, 7, 256)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 7, 7, 256)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 7, 7, 256)    0           activation_16[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 7, 7, 256)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 7, 7, 256)    590080      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 7, 7, 256)    1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 7, 7, 256)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 7, 7, 256)    590080      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 7, 7, 256)    1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 7, 7, 256)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 7, 7, 256)    0           activation_20[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 7, 7, 256)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 4, 4, 512)    1180160     activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 4, 4, 512)    2048        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 512)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 4, 4, 512)    2359808     activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 512)    131584      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 512)    2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 512)    2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 512)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 512)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 512)    0           activation_23[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 512)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 512)    2359808     activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 512)    2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 512)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 512)    2359808     activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 512)    2048        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 512)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 512)    0           activation_27[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 512)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 11,191,938\n",
      "Trainable params: 11,182,338\n",
      "Non-trainable params: 9,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def _after_conv(in_tensor):\n",
    "    norm = layers.BatchNormalization()(in_tensor)\n",
    "    return layers.Activation('relu')(norm)\n",
    "\n",
    "def conv1(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=1, strides=1)(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv1_downsample(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=1, strides=2)(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv3(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv3_downsample(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same')(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def resnet_block_wo_bottlneck(in_tensor, filters, downsample=False):\n",
    "    if downsample:\n",
    "        conv1_rb = conv3_downsample(in_tensor, filters)\n",
    "    else:\n",
    "        conv1_rb = conv3(in_tensor, filters)\n",
    "    conv2_rb = conv3(conv1_rb, filters)\n",
    "\n",
    "    if downsample:\n",
    "        in_tensor = conv1_downsample(in_tensor, filters)\n",
    "    result = layers.Add()([conv2_rb, in_tensor])\n",
    "\n",
    "    return layers.Activation('relu')(result)\n",
    "\n",
    "def resnet_block_w_bottlneck(in_tensor,\n",
    "                             filters,\n",
    "                             downsample=False,\n",
    "                             change_channels=False):\n",
    "    if downsample:\n",
    "        conv1_rb = conv1_downsample(in_tensor, int(filters/4))\n",
    "    else:\n",
    "        conv1_rb = conv1(in_tensor, int(filters/4))\n",
    "    conv2_rb = conv3(conv1_rb, int(filters/4))\n",
    "    conv3_rb = conv1(conv2_rb, filters)\n",
    "\n",
    "    if downsample:\n",
    "        in_tensor = conv1_downsample(in_tensor, filters)\n",
    "    elif change_channels:\n",
    "        in_tensor = conv1(in_tensor, filters)\n",
    "    result = layers.Add()([conv3_rb, in_tensor])\n",
    "\n",
    "    return result\n",
    "\n",
    "def _pre_res_blocks(in_tensor):\n",
    "    conv = layers.Conv2D(64, 7, strides=2, padding='same')(in_tensor)\n",
    "    conv = _after_conv(conv)\n",
    "    pool = layers.MaxPool2D(3, 2, padding='same')(conv)\n",
    "    return pool\n",
    "\n",
    "def _post_res_blocks(in_tensor, n_classes):\n",
    "    pool = layers.GlobalAvgPool2D()(in_tensor)\n",
    "    preds = layers.Dense(n_classes, activation='softmax')(pool)\n",
    "    return preds\n",
    "\n",
    "def convx_wo_bottleneck(in_tensor, filters, n_times, downsample_1=False):\n",
    "    res = in_tensor\n",
    "    for i in range(n_times):\n",
    "        if i == 0:\n",
    "            res = resnet_block_wo_bottlneck(res, filters, downsample_1)\n",
    "        else:\n",
    "            res = resnet_block_wo_bottlneck(res, filters)\n",
    "    return res\n",
    "\n",
    "def convx_w_bottleneck(in_tensor, filters, n_times, downsample_1=False):\n",
    "    res = in_tensor\n",
    "    for i in range(n_times):\n",
    "        if i == 0:\n",
    "            res = resnet_block_w_bottlneck(res, filters, downsample_1, not downsample_1)\n",
    "        else:\n",
    "            res = resnet_block_w_bottlneck(res, filters)\n",
    "    return res\n",
    "\n",
    "def _resnet(in_shape=(224,224,3),\n",
    "            n_classes=1000,\n",
    "            convx=[64, 128, 256, 512],\n",
    "            n_convx=[2, 2, 2, 2],\n",
    "            convx_fn=convx_wo_bottleneck):\n",
    "    in_layer = layers.Input(in_shape)\n",
    "\n",
    "    downsampled = _pre_res_blocks(in_layer)\n",
    "\n",
    "    conv2x = convx_fn(downsampled, convx[0], n_convx[0])\n",
    "    conv3x = convx_fn(conv2x, convx[1], n_convx[1], True)\n",
    "    conv4x = convx_fn(conv3x, convx[2], n_convx[2], True)\n",
    "    conv5x = convx_fn(conv4x, convx[3], n_convx[3], True)\n",
    "\n",
    "    preds = _post_res_blocks(conv5x, n_classes)\n",
    "\n",
    "    model = Model(in_layer, preds)\n",
    "    return model\n",
    "\n",
    "def resnet18(in_shape=(100,100,3), n_classes=2):\n",
    "    return _resnet(in_shape, n_classes)\n",
    "\n",
    "def resnet34(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,n_convx=[3, 4, 6, 3])\n",
    "\n",
    "def resnet50(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 4, 6, 3],convx_w_bottleneck)\n",
    "\n",
    "def resnet101(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 4, 23, 3],convx_w_bottleneck)\n",
    "\n",
    "def resnet152(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 8, 36, 3],convx_w_bottleneck)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = resnet18()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/student/Documents/PCB_3/Models/Res18/Results')\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler,EarlyStopping\n",
    "mc = ModelCheckpoint('PCB3_Res18_Aug_data_100*100_2.h5', monitor='val_loss', mode='min',save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=25, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "cv = CSVLogger('PCB3_Res18_Aug_data_100*100_2.csv',append=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 10:15:16.249745 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 10:15:21.346643 140077872379712 deprecation.py:323] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 10:15:25.761233 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/callbacks.py:848: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W1209 10:15:25.948509 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W1209 10:15:25.955152 140077872379712 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 46s 2ms/step - loss: 0.3870 - acc: 0.8408 - val_loss: 0.2585 - val_acc: 0.9010\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.2357 - acc: 0.9103 - val_loss: 0.7931 - val_acc: 0.7113\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.1955 - acc: 0.9263 - val_loss: 0.1967 - val_acc: 0.9248\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.1738 - acc: 0.9343 - val_loss: 0.1968 - val_acc: 0.9253\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.1598 - acc: 0.9405 - val_loss: 0.2893 - val_acc: 0.9138\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.1436 - acc: 0.9476 - val_loss: 0.1907 - val_acc: 0.9278\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.1265 - acc: 0.9517 - val_loss: 3.7258 - val_acc: 0.7017\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.1227 - acc: 0.9549 - val_loss: 0.2115 - val_acc: 0.9245\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.1280 - acc: 0.9538 - val_loss: 0.1329 - val_acc: 0.9505\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0965 - acc: 0.9630 - val_loss: 0.1274 - val_acc: 0.9520\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0884 - acc: 0.9668 - val_loss: 0.1409 - val_acc: 0.9460\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0808 - acc: 0.9702 - val_loss: 0.1158 - val_acc: 0.9568\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0751 - acc: 0.9718 - val_loss: 0.1364 - val_acc: 0.9477\n",
      "Epoch 14/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0665 - acc: 0.9753 - val_loss: 0.1455 - val_acc: 0.9472\n",
      "Epoch 15/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0585 - acc: 0.9784 - val_loss: 0.1181 - val_acc: 0.9563\n",
      "Epoch 16/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0541 - acc: 0.9797 - val_loss: 0.1260 - val_acc: 0.9590\n",
      "Epoch 17/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0656 - acc: 0.9759 - val_loss: 0.1292 - val_acc: 0.9573\n",
      "Epoch 18/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.0451 - acc: 0.9837 - val_loss: 0.1381 - val_acc: 0.9547\n",
      "Epoch 19/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0415 - acc: 0.9845 - val_loss: 0.2084 - val_acc: 0.9342\n",
      "Epoch 20/100\n",
      "24000/24000 [==============================] - 44s 2ms/step - loss: 0.0456 - acc: 0.9831 - val_loss: 0.1386 - val_acc: 0.9563\n",
      "Epoch 21/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0396 - acc: 0.9855 - val_loss: 0.9541 - val_acc: 0.8640\n",
      "Epoch 22/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0377 - acc: 0.9863 - val_loss: 0.1722 - val_acc: 0.9517\n",
      "Epoch 23/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0291 - acc: 0.9892 - val_loss: 0.1565 - val_acc: 0.9562\n",
      "Epoch 24/100\n",
      "24000/24000 [==============================] - 44s 2ms/step - loss: 0.0295 - acc: 0.9896 - val_loss: 0.1491 - val_acc: 0.9610\n",
      "Epoch 25/100\n",
      "24000/24000 [==============================] - 44s 2ms/step - loss: 0.0270 - acc: 0.9900 - val_loss: 0.1951 - val_acc: 0.9545\n",
      "Epoch 26/100\n",
      "24000/24000 [==============================] - 44s 2ms/step - loss: 0.0284 - acc: 0.9900 - val_loss: 0.1888 - val_acc: 0.9498\n",
      "Epoch 27/100\n",
      "24000/24000 [==============================] - 44s 2ms/step - loss: 0.0232 - acc: 0.9915 - val_loss: 0.2220 - val_acc: 0.9485\n",
      "Epoch 28/100\n",
      "24000/24000 [==============================] - 44s 2ms/step - loss: 0.0274 - acc: 0.9898 - val_loss: 0.1555 - val_acc: 0.9570\n",
      "Epoch 29/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0215 - acc: 0.9918 - val_loss: 0.1654 - val_acc: 0.9580\n",
      "Epoch 30/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0239 - acc: 0.9912 - val_loss: 0.1760 - val_acc: 0.9578\n",
      "Epoch 31/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.0199 - acc: 0.9926 - val_loss: 0.1998 - val_acc: 0.9575\n",
      "Epoch 32/100\n",
      "24000/24000 [==============================] - 45s 2ms/step - loss: 0.0215 - acc: 0.9922 - val_loss: 0.1967 - val_acc: 0.9537\n",
      "Epoch 33/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.2161 - val_acc: 0.9492\n",
      "Epoch 34/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0163 - acc: 0.9942 - val_loss: 0.1895 - val_acc: 0.9568\n",
      "Epoch 35/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0178 - acc: 0.9939 - val_loss: 0.1881 - val_acc: 0.9562\n",
      "Epoch 36/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0386 - acc: 0.9891 - val_loss: 0.1519 - val_acc: 0.9570\n",
      "Epoch 37/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0138 - acc: 0.9950 - val_loss: 0.2024 - val_acc: 0.9560\n",
      "Epoch 38/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.1771 - val_acc: 0.9568\n",
      "Epoch 39/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.0100 - acc: 0.9966 - val_loss: 0.2083 - val_acc: 0.9583\n",
      "Epoch 40/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.0132 - acc: 0.9956 - val_loss: 0.2155 - val_acc: 0.9535\n",
      "Epoch 41/100\n",
      "24000/24000 [==============================] - 42s 2ms/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.2382 - val_acc: 0.9498\n",
      "Epoch 42/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0132 - acc: 0.9953 - val_loss: 0.2362 - val_acc: 0.9525\n",
      "Epoch 43/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.2103 - val_acc: 0.9553\n",
      "Epoch 44/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.2518 - val_acc: 0.9513\n",
      "Epoch 45/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0271 - acc: 0.9922 - val_loss: 0.1773 - val_acc: 0.9538\n",
      "Epoch 46/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.1971 - val_acc: 0.9512\n",
      "Epoch 47/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0159 - acc: 0.9950 - val_loss: 0.1922 - val_acc: 0.9588\n",
      "Epoch 48/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0082 - acc: 0.9972 - val_loss: 0.2069 - val_acc: 0.9603\n",
      "Epoch 49/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0069 - acc: 0.9975 - val_loss: 0.2441 - val_acc: 0.9515\n",
      "Epoch 50/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0089 - acc: 0.9969 - val_loss: 0.2169 - val_acc: 0.9575\n",
      "Epoch 51/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 0.2137 - val_acc: 0.9575\n",
      "Epoch 52/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.2640 - val_acc: 0.9498\n",
      "Epoch 53/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.2148 - val_acc: 0.9568\n",
      "Epoch 54/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0272 - acc: 0.9922 - val_loss: 0.2134 - val_acc: 0.9472\n",
      "Epoch 55/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0091 - acc: 0.9963 - val_loss: 0.2106 - val_acc: 0.9592\n",
      "Epoch 56/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.2736 - val_acc: 0.9528\n",
      "Epoch 57/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0080 - acc: 0.9972 - val_loss: 0.2070 - val_acc: 0.9572\n",
      "Epoch 58/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.2281 - val_acc: 0.9578\n",
      "Epoch 59/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0080 - acc: 0.9977 - val_loss: 0.2601 - val_acc: 0.9532\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0075 - acc: 0.9973 - val_loss: 0.2569 - val_acc: 0.9545\n",
      "Epoch 61/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0105 - acc: 0.9964 - val_loss: 0.1869 - val_acc: 0.9607\n",
      "Epoch 62/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.2767 - val_acc: 0.9405\n",
      "Epoch 63/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.2078 - val_acc: 0.9580\n",
      "Epoch 64/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0073 - acc: 0.9975 - val_loss: 0.2299 - val_acc: 0.9585\n",
      "Epoch 65/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.2284 - val_acc: 0.9580\n",
      "Epoch 66/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.2054 - val_acc: 0.9563\n",
      "Epoch 67/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.2157 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.2129 - val_acc: 0.9577\n",
      "Epoch 69/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.2681 - val_acc: 0.9528\n",
      "Epoch 70/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0092 - acc: 0.9966 - val_loss: 0.2390 - val_acc: 0.9582\n",
      "Epoch 71/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.2154 - val_acc: 0.9577\n",
      "Epoch 72/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0156 - acc: 0.9962 - val_loss: 0.1840 - val_acc: 0.9560\n",
      "Epoch 73/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0117 - acc: 0.9966 - val_loss: 0.2213 - val_acc: 0.9580\n",
      "Epoch 74/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.2551 - val_acc: 0.9563\n",
      "Epoch 75/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.2676 - val_acc: 0.9528\n",
      "Epoch 76/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.2173 - val_acc: 0.9553\n",
      "Epoch 77/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.2859 - val_acc: 0.9538\n",
      "Epoch 78/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0061 - acc: 0.9984 - val_loss: 0.2700 - val_acc: 0.9555\n",
      "Epoch 79/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.2361 - val_acc: 0.9580\n",
      "Epoch 80/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.2347 - val_acc: 0.9588\n",
      "Epoch 81/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.2252 - val_acc: 0.9557\n",
      "Epoch 82/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.2438 - val_acc: 0.9588\n",
      "Epoch 83/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.2164 - val_acc: 0.9590\n",
      "Epoch 84/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.2309 - val_acc: 0.9565\n",
      "Epoch 85/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.2334 - val_acc: 0.9605\n",
      "Epoch 86/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 0.2450 - val_acc: 0.9575\n",
      "Epoch 87/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.2282 - val_acc: 0.9545\n",
      "Epoch 88/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.2516 - val_acc: 0.9602\n",
      "Epoch 89/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.2414 - val_acc: 0.9578\n",
      "Epoch 90/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.2671 - val_acc: 0.9563\n",
      "Epoch 91/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.2254 - val_acc: 0.9577\n",
      "Epoch 92/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.2448 - val_acc: 0.9585\n",
      "Epoch 93/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.2421 - val_acc: 0.9605\n",
      "Epoch 94/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.2382 - val_acc: 0.9558\n",
      "Epoch 95/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.2843 - val_acc: 0.9563\n",
      "Epoch 96/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.2751 - val_acc: 0.9562\n",
      "Epoch 97/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0057 - acc: 0.9978 - val_loss: 0.2952 - val_acc: 0.9567\n",
      "Epoch 98/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.2414 - val_acc: 0.9568\n",
      "Epoch 99/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.2396 - val_acc: 0.9605\n",
      "Epoch 100/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 0.0040 - acc: 0.9989 - val_loss: 0.2328 - val_acc: 0.9587\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_valid,y_valid),verbose = 1,callbacks=[mc,cv,tensorboard_callback],shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
