{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/student/Documents/PCB_3/Train_Data')\n",
    "train_x = np.load('train_X.npy')\n",
    "train_x = train_x[:,100:200,100:200,:];\n",
    "train_y = np.load('train_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([15000, 15000]))\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "print(np.unique(train_y,return_counts=True))\n",
    "train_y= np_utils.to_categorical(train_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n",
      "(30000, 100, 100, 3)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.34926454777778 58.207460392701165\n",
      "(30000, 100, 100, 3)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(train_x)\n",
    "std = np.std(train_x)\n",
    "print (mean, std)\n",
    "\n",
    "train_x = train_x - mean\n",
    "train_x = train_x / std\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "24000\n",
      "\n",
      "6000\n",
      "6000\n",
      "\n",
      "(24000, 100, 100, 3)\n",
      "(24000, 2)\n",
      "\n",
      "(6000, 100, 100, 3)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(len(X_valid))\n",
    "print(len(y_valid))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 100, 100, 3)\n",
      "(24000, 2)\n",
      "\n",
      "(6000, 100, 100, 3)\n",
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "#y_train = np.reshape(y_train,(-1,1))\n",
    "\n",
    "X_valid = X_valid.astype('float32')\n",
    "y_valid = y_valid.astype('float32')\n",
    "#y_valid = np.reshape(y_valid,(-1,1))\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"\")\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1210 14:15:44.971416 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1210 14:15:44.987928 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1210 14:15:44.991526 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1210 14:15:45.014582 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1210 14:15:45.015558 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1210 14:15:45.933198 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1210 14:15:45.990368 140387291580224 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 50, 50, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 50, 50, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 50, 50, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 25, 25, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 25, 25, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 25, 25, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 25, 25, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 25, 25, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 25, 25, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 25, 25, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 25, 25, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 25, 25, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 25, 25, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 25, 25, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 25, 25, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 25, 25, 256)  0           activation_4[0][0]               \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 25, 25, 64)   16448       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 25, 25, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 25, 25, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25, 25, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 256)  16640       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 25, 25, 256)  0           activation_8[0][0]               \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 64)   16448       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 25, 25, 64)   36928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25, 25, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 25, 25, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 25, 256)  16640       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 25, 25, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 25, 25, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 25, 25, 256)  0           activation_11[0][0]              \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 13, 13, 128)  32896       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 13, 13, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 13, 13, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 13, 13, 128)  147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 13, 13, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 13, 13, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 13, 13, 512)  66048       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 13, 13, 512)  131584      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 13, 13, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 13, 13, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 13, 13, 512)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 13, 13, 512)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 13, 13, 512)  0           activation_14[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 13, 13, 128)  65664       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 13, 13, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 13, 13, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 13, 13, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 13, 13, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 13, 13, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 13, 13, 512)  66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 13, 13, 512)  2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 13, 13, 512)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 13, 13, 512)  0           activation_18[0][0]              \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 13, 13, 128)  65664       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 13, 13, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 13, 13, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 13, 13, 128)  147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 13, 13, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 13, 13, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 13, 13, 512)  66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 13, 13, 512)  2048        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 13, 13, 512)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 13, 13, 512)  0           activation_21[0][0]              \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 13, 13, 128)  65664       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 13, 13, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 13, 13, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 13, 13, 128)  147584      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 13, 13, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 13, 13, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 13, 13, 512)  66048       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 13, 13, 512)  2048        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 13, 13, 512)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 13, 13, 512)  0           activation_24[0][0]              \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 7, 7, 256)    131328      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 7, 7, 256)    1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 7, 7, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 7, 7, 256)    590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 7, 7, 256)    1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 7, 7, 256)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 7, 7, 1024)   263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 7, 7, 1024)   525312      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 7, 7, 1024)   4096        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 7, 7, 1024)   4096        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 7, 7, 1024)   0           activation_27[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 256)    262400      add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 7, 7, 256)    1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 7, 7, 256)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 256)    590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 7, 7, 256)    1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 7, 7, 256)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 7, 7, 1024)   263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 7, 7, 1024)   4096        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 7, 7, 1024)   0           activation_31[0][0]              \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 7, 7, 256)    262400      add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 7, 7, 256)    1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 256)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 7, 7, 256)    590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 7, 7, 256)    1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 256)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 7, 7, 1024)   263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 7, 7, 1024)   4096        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 7, 7, 1024)   0           activation_34[0][0]              \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 7, 7, 256)    262400      add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 7, 7, 256)    1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 256)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 7, 7, 256)    590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 256)    1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 256)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 7, 7, 1024)   263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 1024)   4096        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 7, 7, 1024)   0           activation_37[0][0]              \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 7, 7, 256)    262400      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 7, 7, 256)    1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 256)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 7, 7, 256)    590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 7, 7, 256)    1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 7, 7, 256)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 7, 7, 1024)   263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 7, 7, 1024)   4096        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 7, 7, 1024)   0           activation_40[0][0]              \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 7, 7, 256)    262400      add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 7, 7, 256)    1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 256)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 7, 7, 256)    590080      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 7, 7, 256)    1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 256)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 1024)   263168      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 1024)   4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 1024)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 1024)   0           activation_43[0][0]              \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 512)    524800      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 512)    2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 512)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 512)    2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 512)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 2048)   2099200     add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 2048)   8192        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 2048)   8192        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 2048)   0           activation_46[0][0]              \n",
      "                                                                 activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 512)    1049088     add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 512)    2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 512)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 512)    2359808     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 512)    2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 512)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 2048)   1050624     activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 2048)   8192        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 2048)   0           activation_50[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 512)    1049088     add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 512)    2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 512)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 512)    2359808     activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 512)    2048        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 512)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 2048)   1050624     activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 2048)   8192        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 2048)   0           activation_53[0][0]              \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            4098        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 23,538,690\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def _after_conv(in_tensor):\n",
    "    norm = layers.BatchNormalization()(in_tensor)\n",
    "    return layers.Activation('relu')(norm)\n",
    "\n",
    "def conv1(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=1, strides=1)(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv1_downsample(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=1, strides=2)(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv3(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def conv3_downsample(in_tensor, filters):\n",
    "    conv = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same')(in_tensor)\n",
    "    return _after_conv(conv)\n",
    "\n",
    "def resnet_block_wo_bottlneck(in_tensor, filters, downsample=False):\n",
    "    if downsample:\n",
    "        conv1_rb = conv3_downsample(in_tensor, filters)\n",
    "    else:\n",
    "        conv1_rb = conv3(in_tensor, filters)\n",
    "    conv2_rb = conv3(conv1_rb, filters)\n",
    "\n",
    "    if downsample:\n",
    "        in_tensor = conv1_downsample(in_tensor, filters)\n",
    "    result = layers.Add()([conv2_rb, in_tensor])\n",
    "\n",
    "    return layers.Activation('relu')(result)\n",
    "\n",
    "def resnet_block_w_bottlneck(in_tensor,\n",
    "                             filters,\n",
    "                             downsample=False,\n",
    "                             change_channels=False):\n",
    "    if downsample:\n",
    "        conv1_rb = conv1_downsample(in_tensor, int(filters/4))\n",
    "    else:\n",
    "        conv1_rb = conv1(in_tensor, int(filters/4))\n",
    "    conv2_rb = conv3(conv1_rb, int(filters/4))\n",
    "    conv3_rb = conv1(conv2_rb, filters)\n",
    "\n",
    "    if downsample:\n",
    "        in_tensor = conv1_downsample(in_tensor, filters)\n",
    "    elif change_channels:\n",
    "        in_tensor = conv1(in_tensor, filters)\n",
    "    result = layers.Add()([conv3_rb, in_tensor])\n",
    "\n",
    "    return result\n",
    "\n",
    "def _pre_res_blocks(in_tensor):\n",
    "    conv = layers.Conv2D(64, 7, strides=2, padding='same')(in_tensor)\n",
    "    conv = _after_conv(conv)\n",
    "    pool = layers.MaxPool2D(3, 2, padding='same')(conv)\n",
    "    return pool\n",
    "\n",
    "def _post_res_blocks(in_tensor, n_classes):\n",
    "    pool = layers.GlobalAvgPool2D()(in_tensor)\n",
    "    preds = layers.Dense(n_classes, activation='softmax')(pool)\n",
    "    return preds\n",
    "\n",
    "def convx_wo_bottleneck(in_tensor, filters, n_times, downsample_1=False):\n",
    "    res = in_tensor\n",
    "    for i in range(n_times):\n",
    "        if i == 0:\n",
    "            res = resnet_block_wo_bottlneck(res, filters, downsample_1)\n",
    "        else:\n",
    "            res = resnet_block_wo_bottlneck(res, filters)\n",
    "    return res\n",
    "\n",
    "def convx_w_bottleneck(in_tensor, filters, n_times, downsample_1=False):\n",
    "    res = in_tensor\n",
    "    for i in range(n_times):\n",
    "        if i == 0:\n",
    "            res = resnet_block_w_bottlneck(res, filters, downsample_1, not downsample_1)\n",
    "        else:\n",
    "            res = resnet_block_w_bottlneck(res, filters)\n",
    "    return res\n",
    "\n",
    "def _resnet(in_shape=(224,224,3),\n",
    "            n_classes=1000,\n",
    "            convx=[64, 128, 256, 512],\n",
    "            n_convx=[2, 2, 2, 2],\n",
    "            convx_fn=convx_wo_bottleneck):\n",
    "    in_layer = layers.Input(in_shape)\n",
    "\n",
    "    downsampled = _pre_res_blocks(in_layer)\n",
    "\n",
    "    conv2x = convx_fn(downsampled, convx[0], n_convx[0])\n",
    "    conv3x = convx_fn(conv2x, convx[1], n_convx[1], True)\n",
    "    conv4x = convx_fn(conv3x, convx[2], n_convx[2], True)\n",
    "    conv5x = convx_fn(conv4x, convx[3], n_convx[3], True)\n",
    "\n",
    "    preds = _post_res_blocks(conv5x, n_classes)\n",
    "\n",
    "    model = Model(in_layer, preds)\n",
    "    return model\n",
    "\n",
    "def resnet18(in_shape=(100,100,3), n_classes=2):\n",
    "    return _resnet(in_shape, n_classes)\n",
    "\n",
    "def resnet34(in_shape=(100,100,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,n_convx=[3, 4, 6, 3])\n",
    "\n",
    "def resnet50(in_shape=(100,100,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 4, 6, 3],convx_w_bottleneck)\n",
    "\n",
    "def resnet101(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 4, 23, 3],convx_w_bottleneck)\n",
    "\n",
    "def resnet152(in_shape=(300,300,3), n_classes=2):\n",
    "    return _resnet(in_shape,n_classes,[256, 512, 1024, 2048],[3, 8, 36, 3],convx_w_bottleneck)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = resnet50()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/student/Documents/PCB_3/Models/Res50/Results')\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler,EarlyStopping\n",
    "mc = ModelCheckpoint('PCB3_Res50_Aug_data_100*100.h5', monitor='val_loss', mode='min',save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=25, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "cv = CSVLogger('PCB3_Res50_Aug_data_100*100.csv',append=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 06:59:49.399301 140274084452160 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 06:59:53.470415 140274084452160 deprecation.py:323] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 07:00:05.416038 140274084452160 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/callbacks.py:848: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "W1210 07:00:05.858800 140274084452160 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W1210 07:00:05.872195 140274084452160 deprecation_wrapper.py:119] From /home/student/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 111s 5ms/step - loss: 0.6400 - acc: 0.7827 - val_loss: 0.3092 - val_acc: 0.8660\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.3352 - acc: 0.8709 - val_loss: 0.2952 - val_acc: 0.8778\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.2564 - acc: 0.9061 - val_loss: 0.2316 - val_acc: 0.9042\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.2040 - acc: 0.9242 - val_loss: 0.2840 - val_acc: 0.8720\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1829 - acc: 0.9298 - val_loss: 0.6758 - val_acc: 0.8128\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1750 - acc: 0.9335 - val_loss: 0.1960 - val_acc: 0.9237\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1627 - acc: 0.9383 - val_loss: 0.1831 - val_acc: 0.9282\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.1493 - acc: 0.9431 - val_loss: 0.2649 - val_acc: 0.8970\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1343 - acc: 0.9500 - val_loss: 0.1424 - val_acc: 0.9472\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.1727 - acc: 0.9362 - val_loss: 0.2042 - val_acc: 0.9218\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1384 - acc: 0.9480 - val_loss: 0.1490 - val_acc: 0.9433\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.1195 - acc: 0.9548 - val_loss: 0.1378 - val_acc: 0.9447\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1118 - acc: 0.9583 - val_loss: 0.2682 - val_acc: 0.9103\n",
      "Epoch 14/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1071 - acc: 0.9589 - val_loss: 0.1552 - val_acc: 0.9377\n",
      "Epoch 15/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0981 - acc: 0.9635 - val_loss: 0.1699 - val_acc: 0.9383\n",
      "Epoch 16/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0951 - acc: 0.9648 - val_loss: 0.1611 - val_acc: 0.9358\n",
      "Epoch 17/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0885 - acc: 0.9670 - val_loss: 0.1192 - val_acc: 0.9538\n",
      "Epoch 18/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0852 - acc: 0.9687 - val_loss: 0.1200 - val_acc: 0.9538\n",
      "Epoch 19/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0774 - acc: 0.9707 - val_loss: 0.1223 - val_acc: 0.9533\n",
      "Epoch 20/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0914 - acc: 0.9662 - val_loss: 0.1235 - val_acc: 0.9585\n",
      "Epoch 21/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0696 - acc: 0.9746 - val_loss: 0.1346 - val_acc: 0.9557\n",
      "Epoch 22/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0617 - acc: 0.9772 - val_loss: 0.1336 - val_acc: 0.9560\n",
      "Epoch 23/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0799 - acc: 0.9687 - val_loss: 0.1633 - val_acc: 0.9465\n",
      "Epoch 24/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0581 - acc: 0.9791 - val_loss: 0.1216 - val_acc: 0.9587\n",
      "Epoch 25/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0525 - acc: 0.9800 - val_loss: 0.1485 - val_acc: 0.9525\n",
      "Epoch 26/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0457 - acc: 0.9820 - val_loss: 0.1646 - val_acc: 0.9452\n",
      "Epoch 27/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.2130 - acc: 0.9479 - val_loss: 1.1933 - val_acc: 0.6138\n",
      "Epoch 28/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.1599 - acc: 0.9366 - val_loss: 0.1331 - val_acc: 0.9512\n",
      "Epoch 29/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0896 - acc: 0.9657 - val_loss: 0.1191 - val_acc: 0.9543\n",
      "Epoch 30/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0632 - acc: 0.9767 - val_loss: 0.1427 - val_acc: 0.9503\n",
      "Epoch 31/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0564 - acc: 0.9797 - val_loss: 0.1363 - val_acc: 0.9555\n",
      "Epoch 32/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0461 - acc: 0.9833 - val_loss: 0.1369 - val_acc: 0.9558\n",
      "Epoch 33/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0419 - acc: 0.9848 - val_loss: 0.1427 - val_acc: 0.9535\n",
      "Epoch 34/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0378 - acc: 0.9867 - val_loss: 0.1437 - val_acc: 0.9573\n",
      "Epoch 35/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.1701 - val_acc: 0.9497\n",
      "Epoch 36/100\n",
      "24000/24000 [==============================] - 103s 4ms/step - loss: 0.0355 - acc: 0.9871 - val_loss: 0.1526 - val_acc: 0.9565\n",
      "Epoch 37/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0354 - acc: 0.9873 - val_loss: 0.1870 - val_acc: 0.9525\n",
      "Epoch 38/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0326 - acc: 0.9878 - val_loss: 0.1490 - val_acc: 0.9560\n",
      "Epoch 39/100\n",
      "24000/24000 [==============================] - 103s 4ms/step - loss: 0.0376 - acc: 0.9871 - val_loss: 0.1513 - val_acc: 0.9575\n",
      "Epoch 40/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0277 - acc: 0.9903 - val_loss: 0.1648 - val_acc: 0.9567\n",
      "Epoch 41/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0305 - acc: 0.9887 - val_loss: 0.1640 - val_acc: 0.9537\n",
      "Epoch 42/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0256 - acc: 0.9917 - val_loss: 0.1970 - val_acc: 0.9563\n",
      "Epoch 43/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0264 - acc: 0.9902 - val_loss: 0.2030 - val_acc: 0.9538\n",
      "Epoch 44/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0251 - acc: 0.9912 - val_loss: 0.1712 - val_acc: 0.9563\n",
      "Epoch 45/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0241 - acc: 0.9911 - val_loss: 0.1669 - val_acc: 0.9582\n",
      "Epoch 46/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0270 - acc: 0.9903 - val_loss: 0.1590 - val_acc: 0.9612\n",
      "Epoch 47/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0231 - acc: 0.9922 - val_loss: 0.1706 - val_acc: 0.9602\n",
      "Epoch 48/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0206 - acc: 0.9924 - val_loss: 0.1931 - val_acc: 0.9557\n",
      "Epoch 49/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0221 - acc: 0.9917 - val_loss: 0.1892 - val_acc: 0.9575\n",
      "Epoch 50/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0201 - acc: 0.9927 - val_loss: 0.2200 - val_acc: 0.9510\n",
      "Epoch 51/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.1939 - val_acc: 0.9600\n",
      "Epoch 52/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0183 - acc: 0.9940 - val_loss: 0.2028 - val_acc: 0.9552\n",
      "Epoch 53/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0202 - acc: 0.9929 - val_loss: 0.1641 - val_acc: 0.9613\n",
      "Epoch 54/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0166 - acc: 0.9937 - val_loss: 0.2121 - val_acc: 0.9513\n",
      "Epoch 55/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0186 - acc: 0.9935 - val_loss: 0.1704 - val_acc: 0.9592\n",
      "Epoch 56/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0143 - acc: 0.9953 - val_loss: 0.1992 - val_acc: 0.9597\n",
      "Epoch 57/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0147 - acc: 0.9946 - val_loss: 0.2124 - val_acc: 0.9590\n",
      "Epoch 58/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0156 - acc: 0.9943 - val_loss: 0.1898 - val_acc: 0.9578\n",
      "Epoch 59/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0124 - acc: 0.9951 - val_loss: 0.2695 - val_acc: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0153 - acc: 0.9942 - val_loss: 0.1990 - val_acc: 0.9588\n",
      "Epoch 61/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0114 - acc: 0.9956 - val_loss: 0.2252 - val_acc: 0.9585\n",
      "Epoch 62/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0157 - acc: 0.9944 - val_loss: 0.1957 - val_acc: 0.9632\n",
      "Epoch 63/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0138 - acc: 0.9950 - val_loss: 0.1706 - val_acc: 0.9578\n",
      "Epoch 64/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0130 - acc: 0.9955 - val_loss: 0.2164 - val_acc: 0.9557\n",
      "Epoch 65/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0132 - acc: 0.9949 - val_loss: 0.2354 - val_acc: 0.9517\n",
      "Epoch 66/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0147 - acc: 0.9950 - val_loss: 0.2040 - val_acc: 0.9607\n",
      "Epoch 67/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0105 - acc: 0.9963 - val_loss: 0.2405 - val_acc: 0.9560\n",
      "Epoch 68/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0317 - acc: 0.9902 - val_loss: 0.1882 - val_acc: 0.9568\n",
      "Epoch 69/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0126 - acc: 0.9958 - val_loss: 0.2112 - val_acc: 0.9590\n",
      "Epoch 70/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0076 - acc: 0.9972 - val_loss: 0.2412 - val_acc: 0.9542\n",
      "Epoch 71/100\n",
      "24000/24000 [==============================] - 99s 4ms/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.2278 - val_acc: 0.9588\n",
      "Epoch 72/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0144 - acc: 0.9950 - val_loss: 0.2136 - val_acc: 0.9543\n",
      "Epoch 73/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.1964 - val_acc: 0.9603\n",
      "Epoch 74/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.1922 - val_acc: 0.9603\n",
      "Epoch 75/100\n",
      "24000/24000 [==============================] - 98s 4ms/step - loss: 0.0144 - acc: 0.9948 - val_loss: 0.1952 - val_acc: 0.9598\n",
      "Epoch 76/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.2353 - val_acc: 0.9557\n",
      "Epoch 77/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.2270 - val_acc: 0.9598\n",
      "Epoch 78/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0082 - acc: 0.9971 - val_loss: 0.2329 - val_acc: 0.9588\n",
      "Epoch 79/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0105 - acc: 0.9965 - val_loss: 0.1976 - val_acc: 0.9547\n",
      "Epoch 80/100\n",
      "24000/24000 [==============================] - 102s 4ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.1916 - val_acc: 0.9603\n",
      "Epoch 81/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.2597 - val_acc: 0.9525\n",
      "Epoch 82/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.2072 - val_acc: 0.9590\n",
      "Epoch 83/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.2736 - val_acc: 0.9528\n",
      "Epoch 84/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0096 - acc: 0.9969 - val_loss: 0.2106 - val_acc: 0.9597\n",
      "Epoch 85/100\n",
      "24000/24000 [==============================] - 100s 4ms/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.2482 - val_acc: 0.9558\n",
      "Epoch 86/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.2374 - val_acc: 0.9610\n",
      "Epoch 87/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.2960 - val_acc: 0.9588\n",
      "Epoch 88/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0119 - acc: 0.9958 - val_loss: 0.2154 - val_acc: 0.9583\n",
      "Epoch 89/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.2360 - val_acc: 0.9593\n",
      "Epoch 90/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.2552 - val_acc: 0.9560\n",
      "Epoch 91/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.2267 - val_acc: 0.9607\n",
      "Epoch 92/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.2445 - val_acc: 0.9573\n",
      "Epoch 93/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.2429 - val_acc: 0.9582\n",
      "Epoch 94/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0074 - acc: 0.9979 - val_loss: 0.2823 - val_acc: 0.9485\n",
      "Epoch 95/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.2323 - val_acc: 0.9585\n",
      "Epoch 96/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.2239 - val_acc: 0.9608\n",
      "Epoch 97/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.2772 - val_acc: 0.9480\n",
      "Epoch 98/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.2694 - val_acc: 0.9547\n",
      "Epoch 99/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.2517 - val_acc: 0.9535\n",
      "Epoch 100/100\n",
      "24000/24000 [==============================] - 101s 4ms/step - loss: 0.0075 - acc: 0.9979 - val_loss: 0.2585 - val_acc: 0.9592\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_valid,y_valid),verbose = 1,callbacks=[mc,cv,tensorboard_callback],shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
